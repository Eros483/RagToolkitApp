{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab09bb3",
   "metadata": {},
   "source": [
    "All necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c5c9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnab\\miniconda3\\envs\\ragEnv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "import llama_cpp\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a79533",
   "metadata": {},
   "source": [
    "Path of pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2010b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"D:\\\\personalCode\\\\RAG-RUNNER\\\\documents\\\\sample.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7752956",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_json=\"D:\\\\personalCode\\\\RAG-RUNNER\\\\metrics\\\\sample1.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36529c",
   "metadata": {},
   "source": [
    "PDF text reading function, text splitting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1753748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_text(path):\n",
    "    doc=pymupdf.open(path)\n",
    "    full_text=\"\"\n",
    "    for page in doc:\n",
    "        full_text+=page.get_text()\n",
    "    return full_text\n",
    "\n",
    "def split_into_chunks(text, chunk_size=500, overlap=50):\n",
    "    chunks=[]\n",
    "    for i in range(0, len(text), chunk_size-overlap):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96185681",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=read_pdf_text(path)\n",
    "chunks=split_into_chunks(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478da314",
   "metadata": {},
   "source": [
    "Embedding and storage of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9803b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder=SentenceTransformer('all-MiniLM-L6-v2')\n",
    "vectors=embedder.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f05b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension=vectors[0].shape[0]\n",
    "index=faiss.IndexFlatL2(dimension)\n",
    "index.reset()\n",
    "index.add(np.array(vectors))\n",
    "\n",
    "id_to_text={i: chunk for i, chunk in enumerate(chunks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11de15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chunks(query, top_k=3):\n",
    "    query_vec=embedder.encode([query])\n",
    "    D, I=index.search(np.array(query_vec), top_k)\n",
    "    return [id_to_text[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d05f5f",
   "metadata": {},
   "source": [
    "Model being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "affea059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e509a7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4060 Laptop GPU) - 3535 MiB free\n",
      "llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from D:\\\\personalCode\\\\RAG-runner-deployment\\\\backend\\\\models\\\\Llama-3.2-8B-Instruct-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.1 8B Instruct\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Meta Llama\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Llama-3.1\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q5_K - Medium\n",
      "print_info: file size   = 5.33 GiB (5.70 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = Llama 3.1 8B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q5_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
      "load_tensors: offloading 32 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 33/33 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =  5115.49 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   344.44 MiB\n",
      "........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 8192\n",
      "llama_context: n_ctx_per_seq = 8192\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 8192 (padded)\n",
      "llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  28: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  29: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  30: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  31: dev = CUDA0\n",
      "llama_kv_cache_unified:      CUDA0 KV buffer size =  1024.00 MiB\n",
      "llama_kv_cache_unified: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 2\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:      CUDA0 compute buffer size =   560.00 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =    24.01 MiB\n",
      "llama_context: graph nodes  = 1094\n",
      "llama_context: graph splits = 2\n",
      "CUDA : ARCHS = 890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Llama 3.1 8B Instruct', 'general.architecture': 'llama', 'general.type': 'model', 'llama.context_length': '131072', 'general.organization': 'Meta Llama', 'llama.block_count': '32', 'general.basename': 'Llama-3.1', 'general.finetune': 'Instruct', 'general.size_label': '8B', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '17', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '128', 'llama.attention.value_length': '128', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    }
   ],
   "source": [
    "model_path_gguf=r\"D:\\\\personalCode\\\\RAG-runner-deployment\\\\backend\\\\models\\\\Llama-3.2-8B-Instruct-Q5_K_M.gguf\"\n",
    "model=llama_cpp.Llama(model_path=model_path_gguf, chat_format=\"llama-2\", n_ctx=8192, n_gpu_layers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c891863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n"
     ]
    }
   ],
   "source": [
    "print(model.context_params.n_ctx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6464325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1586.90 ms\n",
      "llama_perf_context_print: prompt eval time =    1586.13 ms /   122 tokens (   13.00 ms per token,    76.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10062.59 ms /   129 runs   (   78.00 ms per token,    12.82 tokens per second)\n",
      "llama_perf_context_print:       total time =   11788.48 ms /   251 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------\n",
      "\n",
      "வணக்கம், நான் அர்நாப், கணினி அறிவியல் மற்றும் பொறியியல் துறையில் பட்டதாரி படிப்பை செய்கிறேன். <|im_end|>\n",
      "\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query=\"Hello, I am arnab, a CSE major doing Bachelors in technology.\"\n",
    "\n",
    "tamil_prompt=f\"\"\"<|im_start|>system\n",
    "You are a dedicated english to tamil translator, whose job is to translate the user's {query} into tamil language, using tamil characters.\n",
    "Ensure the output is only in tamil, and is grammatically correct.\n",
    "Do not add unnecessary decorators.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "temp=0.7\n",
    "max_tokens=512\n",
    "\n",
    "response=model.create_completion(\n",
    "    prompt=tamil_prompt,\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens\n",
    ")\n",
    "\n",
    "assistant_reply=response['choices'][0]['text']\n",
    "assistant_reply=assistant_reply.replace(\"[/INST]\", \"\")\n",
    "print(\"\\n-----------------------------------\\n\")\n",
    "print(assistant_reply)\n",
    "print(\"\\n-----------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e07c73e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (AI) and its impact on various industries.\n",
      "\n",
      "நான் அறிந்துள்ளது பொருள் நிலையில் இரு அணுக்கைகள் உண்டு. முதல் அணுக்கை என்று அறியப்படும் இயக்கம் இன்றளவு வாழ்கிறது. இது இயக்கத்தின் ஒரு வடிவம் ஆகும். இது வாழ்கிறது என்பதற்கு காரணம் இயக்கத்தின் நிலைவு ஆகும். இயக்கத்தின் நிலைவு இயக்கத்தின் வடிவங்களில் இரு வடிவங்கள் உண்டு. இயக்கம் வாழ்கிறது என்பதற்கு காரணம் இயக்கத்தின் நிலைவு ஆகும். இயக்கம் வாழ்கிறது என்பதற்கு காரணம் இயக்கத்தின் நிலைவு\n"
     ]
    }
   ],
   "source": [
    "assistant_reply=response['choices'][0]['text']\n",
    "assistant_reply=assistant_reply.replace(\"[/INST]\", \"\")\n",
    "print(assistant_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de4bad",
   "metadata": {},
   "source": [
    "User input and search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9565e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens used by the context: 437\n"
     ]
    }
   ],
   "source": [
    "user_query=input(\"Please enter query: \")\n",
    "context=\"\\n\\n\".join(search_chunks(user_query))\n",
    "\n",
    "context_tokens = model.tokenize(context.encode(\"utf-8\"))\n",
    "context_token_count = len(context_tokens)\n",
    "print(f\"Total tokens used by the context: {context_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b22b67a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'student': {'id': 'S12345', 'name': 'Arnab', 'age': 19, 'date': '2025-05-20'}, 'test_context': {'type': 'Fitness Evaluation', 'source_pdf': 'fitness_assessment_protocols_vr2025.pdf', 'description': 'VR simulation mimicking physical fitness tests based on standard protocols. Includes a theoretical quiz.'}, 'vr_simulation_results': {'session_duration_seconds': 900, 'metrics': {'cardio_endurance_score': 78, 'muscular_strength_score': 84, 'reaction_time_ms': 245, 'balance_score': 90, 'flexibility_score': 72, 'fatigue_index': 0.18, 'errors_made': 1, 'tasks_completed': 10, 'avg_heart_rate_bpm': 138}, 'engagement_level': 'high', 'instructor_comments': 'Impressive balance and strength. Could improve flexibility routines.'}, 'quiz_results': {'topic': 'Basic Fitness & Anatomy', 'total_questions': 15, 'correct_answers': 13, 'score_percentage': 86.7, 'time_taken_seconds': 410, 'difficulty_level': 'medium', 'remarks': 'Strong grasp on physiology; missed a couple on stretching techniques.'}, 'rag_agent_summary': {'insights_generated': True, 'summary': 'Arnab demonstrates strong overall fitness with excellent performance in strength and balance tests. Quiz results reflect good theoretical understanding. Slight improvement needed in flexibility training and injury prevention knowledge.', 'recommendations': ['Add dynamic stretching to warm-ups.', 'Review injury prevention protocols in the provided material.', 'Retest in 2 weeks to track flexibility gains.']}}\n"
     ]
    }
   ],
   "source": [
    "with open(path_json, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d69f389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     774.31 ms\n",
      "llama_perf_context_print: prompt eval time =     764.54 ms /   939 tokens (    0.81 ms per token,  1228.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6906.68 ms /   511 runs   (   13.52 ms per token,    73.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    8325.97 ms /  1450 tokens\n"
     ]
    }
   ],
   "source": [
    "final_prompt=f\"\"\"<|im_start|>system\n",
    "You are a helpful assistant in a set up, where the task is generate evaluation feedback. Consider the metrics to contain information on the conducted evaluation. Use the added context, to enhance the answers created from the metrics. If the answer is not present in the context, print \"Insufficient context\" and nothing else. Structure your response in markdown, using bullet points or headings if appropriate. Ensure that if there is no relevant information, you provide \"Insufficient context\" and nothing else at all. <|im_end|>\n",
    "<|im_start|>user\n",
    "Use the following metrics to answer the question. Enhance the answer using the given context.\n",
    "\n",
    "Metrics:\n",
    "{data}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{user_query}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "temp=0.7\n",
    "max_tokens=512\n",
    "\n",
    "response=model.create_completion(\n",
    "    prompt=final_prompt,\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb3713d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown:\n",
      "\n",
      "**How to do a Push Up?**\n",
      "\n",
      "- **Positioning**: \n",
      "  - Begin in a high plank position with your hands shoulder-width apart. \n",
      "  - Keep your feet together and your body in a straight line, with your head directly in line with your hands.\n",
      "  - Your body should form a straight line from your head to your feet, with a slight bend in your knees.\n",
      "\n",
      "- **Engagement**: \n",
      "  - **Shoulders**: \n",
      "    - Pull your shoulder blades together and back to engage your upper back and shoulder muscles.\n",
      "    - Imagine you’re trying to touch the opposite shoulder with your elbow.\n",
      "    - **Pectorals**: \n",
      "    - Keep your chest engaged by pulling your chest muscles together and up.\n",
      "    - **Core**: \n",
      "    - Tighten your abdominal muscles to protect your lower back.\n",
      "    - **Triceps and Deltoids**: \n",
      "    - Maintain a straight arm position, bending your elbows just enough to form a 90-degree angle at the top of your movement.\n",
      "\n",
      "- **Movement**: \n",
      "  - **Lower**: \n",
      "    - Start by lowering your body until your chest is just above the ground, with your elbows at a 45-degree angle.\n",
      "    - Keep your core tight and your body straight. \n",
      "  - **Push**: \n",
      "    - Push yourself up by extending your elbows, straightening your arms, and lifting your body back to the starting position.\n",
      "    - Keep your body straight, shoulders down, and chest up.\n",
      "\n",
      "- **Variations**: \n",
      "  - **Elevated Surface**: \n",
      "    - Start with your hands on a raised surface, such as a table, bureau, armrest of a futon or couch, or a wall.\n",
      "    - The higher the surface, the easier the push up will be.\n",
      "\n",
      "- **Tips**: \n",
      "  - **Gradual Increase**: \n",
      "    - Gradually increase the difficulty of your push ups by lowering the surface or by increasing the range of motion.\n",
      "    - For example, pull a chair close to a couch or futon armrest, put your hands on the armrest, put your feet up on the chair, etc.\n",
      "  - **Power**: \n",
      "    - Push yourself up so hard and fast that your hands come up off the ground at the top of the movement.\n",
      "    - This will help build explosive power.\n",
      "\n",
      "- **Safety**: \n",
      "  - **Incorrect Form**: \n",
      "    - Avoid bending your elbows too much or rounding your spine to decrease the stress on your back.\n",
      "    - This can help prevent lower\n"
     ]
    }
   ],
   "source": [
    "assistant_reply=response['choices'][0]['text']\n",
    "assistant_reply=assistant_reply.replace(\"[/INST]\", \"\")\n",
    "print(assistant_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1831027f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 605 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     778.50 ms\n",
      "llama_perf_context_print: prompt eval time =     336.30 ms /   605 tokens (    0.56 ms per token,  1799.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =     117.94 ms /     7 runs   (   16.85 ms per token,    59.35 tokens per second)\n",
      "llama_perf_context_print:       total time =     462.76 ms /   612 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant reply is not hallucinated.\n"
     ]
    }
   ],
   "source": [
    "hallucination_prompt = f\"\"\"<|im_start|>system\n",
    "You are an expert fact-checking assistant. Your job is to verify whether the assistant's answer is fully supported by the given context. If parts of the answer are not present in the context, clearly identify them. Be strict and objective in your judgment.<|im_end|>\n",
    "<|im_start|>user\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "{assistant_reply}\n",
    "\n",
    "Task:\n",
    "Determine whether the answer is hallucinated. List any parts of the answer that are not supported by the context. If answer is determined to not be hallucinated respond with \"Assistant reply is not hallucinated.\"<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "hal_response = model.create_completion(\n",
    "    prompt=hallucination_prompt,\n",
    "    temperature=temp,\n",
    "    max_tokens=256\n",
    ")\n",
    "assistant_hallucination = hal_response['choices'][0]['text']\n",
    "print(assistant_hallucination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76adb32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant reply is not hallucinated.\n"
     ]
    }
   ],
   "source": [
    "print(assistant_hallucination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd517867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1f47878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_semantic_entropy(responses, distance_threshold=1.0):\n",
    "    embeddings=embedder.encode(responses)\n",
    "\n",
    "    clustering=AgglomerativeClustering(n_clusters=None, distance_threshold=distance_threshold, linkage=\"average\")\n",
    "    labels=clustering.fit_predict(embeddings)\n",
    "\n",
    "    counts=Counter(labels)\n",
    "    total=sum(counts.values())\n",
    "    probabilities=[count/total for count in counts.values()]\n",
    "    entropy=-sum(p*math.log2(p) for p in probabilities)\n",
    "\n",
    "    return entropy, labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "003b2fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =     222.18 ms /     7 tokens (   31.74 ms per token,    31.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1908.22 ms /   138 runs   (   13.83 ms per token,    72.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    2259.40 ms /   145 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2588.46 ms /   187 runs   (   13.84 ms per token,    72.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    2765.17 ms /   188 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3565.64 ms /   256 runs   (   13.93 ms per token,    71.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    3819.42 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2641.84 ms /   192 runs   (   13.76 ms per token,    72.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    2818.49 ms /   193 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3562.41 ms /   256 runs   (   13.92 ms per token,    71.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    3820.55 ms /   257 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3236.62 ms /   223 runs   (   14.51 ms per token,    68.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    3451.05 ms /   224 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3557.23 ms /   256 runs   (   13.90 ms per token,    71.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    3799.40 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3130.27 ms /   226 runs   (   13.85 ms per token,    72.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    3343.91 ms /   227 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3581.14 ms /   256 runs   (   13.99 ms per token,    71.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    3795.97 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3554.61 ms /   254 runs   (   13.99 ms per token,    71.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    3757.32 ms /   255 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 1 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2982.39 ms /   214 runs   (   13.94 ms per token,    71.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    3152.50 ms /   215 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3584.33 ms /   256 runs   (   14.00 ms per token,    71.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    3784.38 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3585.11 ms /   256 runs   (   14.00 ms per token,    71.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    3784.94 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3578.12 ms /   256 runs   (   13.98 ms per token,    71.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    3782.99 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1223.05 ms /    89 runs   (   13.74 ms per token,    72.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1286.52 ms /    90 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 2 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2632.55 ms /   190 runs   (   13.86 ms per token,    72.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    2779.57 ms /   191 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2236.80 ms /   162 runs   (   13.81 ms per token,    72.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    2359.78 ms /   163 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3570.30 ms /   256 runs   (   13.95 ms per token,    71.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    3781.11 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3563.60 ms /   255 runs   (   13.97 ms per token,    71.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    3774.99 ms /   256 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1428.44 ms /   104 runs   (   13.74 ms per token,    72.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1503.04 ms /   105 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 3 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3253.38 ms /   234 runs   (   13.90 ms per token,    71.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    3436.15 ms /   235 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2197.26 ms /   159 runs   (   13.82 ms per token,    72.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    2314.72 ms /   160 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     176.75 ms /    13 runs   (   13.60 ms per token,    73.55 tokens per second)\n",
      "llama_perf_context_print:       total time =     185.56 ms /    14 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1317.13 ms /    96 runs   (   13.72 ms per token,    72.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1385.79 ms /    97 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1499.05 ms /   109 runs   (   13.75 ms per token,    72.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1577.11 ms /   110 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 4 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1929.01 ms /   140 runs   (   13.78 ms per token,    72.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    2033.87 ms /   141 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2936.50 ms /   211 runs   (   13.92 ms per token,    71.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    3100.38 ms /   212 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1320.17 ms /    96 runs   (   13.75 ms per token,    72.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1388.05 ms /    97 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3578.36 ms /   256 runs   (   13.98 ms per token,    71.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    3788.03 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3572.97 ms /   256 runs   (   13.96 ms per token,    71.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    3777.72 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 5 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3230.95 ms /   232 runs   (   13.93 ms per token,    71.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    3414.69 ms /   233 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.82 ms /    84 runs   (   13.75 ms per token,    72.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1214.61 ms /    85 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3577.89 ms /   256 runs   (   13.98 ms per token,    71.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    3787.64 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3108.11 ms /   223 runs   (   13.94 ms per token,    71.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    3280.89 ms /   224 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3592.92 ms /   256 runs   (   14.03 ms per token,    71.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    3801.13 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 6 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2019.33 ms /   146 runs   (   13.83 ms per token,    72.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    2130.97 ms /   147 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1684.30 ms /   122 runs   (   13.81 ms per token,    72.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1775.65 ms /   123 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2079.33 ms /   150 runs   (   13.86 ms per token,    72.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    2192.38 ms /   151 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1984.70 ms /   143 runs   (   13.88 ms per token,    72.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    2088.58 ms /   144 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1401.14 ms /   101 runs   (   13.87 ms per token,    72.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1480.99 ms /   102 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 7 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2263.96 ms /   163 runs   (   13.89 ms per token,    72.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    2410.65 ms /   164 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3074.45 ms /   220 runs   (   13.97 ms per token,    71.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    3283.07 ms /   221 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3591.14 ms /   256 runs   (   14.03 ms per token,    71.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    3839.09 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3588.13 ms /   256 runs   (   14.02 ms per token,    71.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    3834.01 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3591.48 ms /   256 runs   (   14.03 ms per token,    71.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    3848.05 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 8 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3585.29 ms /   256 runs   (   14.01 ms per token,    71.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    3837.25 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3590.72 ms /   256 runs   (   14.03 ms per token,    71.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    3844.73 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3601.37 ms /   256 runs   (   14.07 ms per token,    71.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    3868.24 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3592.33 ms /   256 runs   (   14.03 ms per token,    71.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3854.36 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2516.77 ms /   180 runs   (   13.98 ms per token,    71.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    2682.76 ms /   181 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 9 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3607.52 ms /   256 runs   (   14.09 ms per token,    70.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    3875.55 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3598.81 ms /   256 runs   (   14.06 ms per token,    71.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    3855.79 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2300.62 ms /   165 runs   (   13.94 ms per token,    71.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    2456.06 ms /   166 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2984.05 ms /   213 runs   (   14.01 ms per token,    71.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    3190.29 ms /   214 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      14.86 ms /     1 runs   (   14.86 ms per token,    67.31 tokens per second)\n",
      "llama_perf_context_print:       total time =      16.48 ms /     2 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 10 Entropy:0.7219280948873623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     827.37 ms /    60 runs   (   13.79 ms per token,    72.52 tokens per second)\n",
      "llama_perf_context_print:       total time =     876.89 ms /    61 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      14.82 ms /     1 runs   (   14.82 ms per token,    67.46 tokens per second)\n",
      "llama_perf_context_print:       total time =      16.04 ms /     2 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3604.43 ms /   256 runs   (   14.08 ms per token,    71.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    3863.16 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3600.39 ms /   256 runs   (   14.06 ms per token,    71.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    3858.29 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1389.07 ms /   100 runs   (   13.89 ms per token,    71.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1477.25 ms /   101 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 11 Entropy:0.7219280948873623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3608.45 ms /   256 runs   (   14.10 ms per token,    70.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    3864.64 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1934.25 ms /   139 runs   (   13.92 ms per token,    71.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    2040.48 ms /   140 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3595.05 ms /   255 runs   (   14.10 ms per token,    70.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    3815.45 ms /   256 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1086.23 ms /    78 runs   (   13.93 ms per token,    71.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1145.08 ms /    79 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1939.02 ms /   139 runs   (   13.95 ms per token,    71.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    2046.50 ms /   140 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 12 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3605.01 ms /   256 runs   (   14.08 ms per token,    71.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    3849.74 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3605.75 ms /   256 runs   (   14.08 ms per token,    71.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    3855.38 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3606.30 ms /   256 runs   (   14.09 ms per token,    70.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    3854.07 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1469.25 ms /   106 runs   (   13.86 ms per token,    72.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1561.84 ms /   107 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3596.49 ms /   256 runs   (   14.05 ms per token,    71.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    3840.27 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 13 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3137.86 ms /   224 runs   (   14.01 ms per token,    71.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    3350.18 ms /   225 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3602.21 ms /   256 runs   (   14.07 ms per token,    71.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    3852.78 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2218.31 ms /   159 runs   (   13.95 ms per token,    71.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    2360.26 ms /   160 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2198.36 ms /   158 runs   (   13.91 ms per token,    71.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    2341.64 ms /   159 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3599.88 ms /   256 runs   (   14.06 ms per token,    71.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    3847.07 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 14 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3601.87 ms /   256 runs   (   14.07 ms per token,    71.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    3850.62 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2148.12 ms /   154 runs   (   13.95 ms per token,    71.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    2285.15 ms /   155 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3600.95 ms /   256 runs   (   14.07 ms per token,    71.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    3851.25 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3013.54 ms /   215 runs   (   14.02 ms per token,    71.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3212.89 ms /   216 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3598.86 ms /   256 runs   (   14.06 ms per token,    71.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    3854.68 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 15 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1439.23 ms /   104 runs   (   13.84 ms per token,    72.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1530.57 ms /   105 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3288.23 ms /   234 runs   (   14.05 ms per token,    71.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    3510.19 ms /   235 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3602.59 ms /   256 runs   (   14.07 ms per token,    71.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    3854.29 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2789.78 ms /   199 runs   (   14.02 ms per token,    71.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    2975.38 ms /   200 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3605.64 ms /   256 runs   (   14.08 ms per token,    71.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    3852.47 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 16 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3601.93 ms /   256 runs   (   14.07 ms per token,    71.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    3858.30 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2219.24 ms /   159 runs   (   13.96 ms per token,    71.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    2365.83 ms /   160 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1712.87 ms /   123 runs   (   13.93 ms per token,    71.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1821.86 ms /   124 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3604.09 ms /   256 runs   (   14.08 ms per token,    71.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    3844.66 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3275.12 ms /   233 runs   (   14.06 ms per token,    71.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    3497.38 ms /   234 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 17 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2354.40 ms /   169 runs   (   13.93 ms per token,    71.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    2501.80 ms /   170 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2819.26 ms /   201 runs   (   14.03 ms per token,    71.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    3006.78 ms /   202 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2800.15 ms /   200 runs   (   14.00 ms per token,    71.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    2983.04 ms /   201 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1333.22 ms /    96 runs   (   13.89 ms per token,    72.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1414.91 ms /    97 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3596.49 ms /   256 runs   (   14.05 ms per token,    71.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    3836.26 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 18 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3366.06 ms /   240 runs   (   14.03 ms per token,    71.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    3557.72 ms /   241 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3598.86 ms /   256 runs   (   14.06 ms per token,    71.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    3803.10 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2656.86 ms /   190 runs   (   13.98 ms per token,    71.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    2802.41 ms /   191 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3604.93 ms /   256 runs   (   14.08 ms per token,    71.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    3828.84 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     907.49 ms /    66 runs   (   13.75 ms per token,    72.73 tokens per second)\n",
      "llama_perf_context_print:       total time =     953.45 ms /    67 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 19 Entropy:-0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    semantic_response=[model.create_completion(prompt=user_query, temperature=temp, max_tokens=256)['choices'][0]['text'] for _ in range(5)]\n",
    "    entropy, labels=compute_semantic_entropy(semantic_response, distance_threshold=1.0)\n",
    "    print(f\"At iteration {i} Entropy:{entropy}\")\n",
    "    if entropy>=1.5:\n",
    "        print(f\"iteration {i} detected possible hallucination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d0b2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 554 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =   14753.43 ms /   554 tokens (   26.63 ms per token,    37.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21498.60 ms /   255 runs   (   84.31 ms per token,    11.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   36652.24 ms /   809 tokens\n",
      "Llama.generate: 559 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     379.05 ms /     4 runs   (   94.76 ms per token,    10.55 tokens per second)\n",
      "llama_perf_context_print:       total time =     385.26 ms /     5 tokens\n",
      "Llama.generate: 559 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   21344.26 ms /   256 runs   (   83.38 ms per token,    11.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   21743.95 ms /   257 tokens\n",
      "Llama.generate: 559 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     338.12 ms /     4 runs   (   84.53 ms per token,    11.83 tokens per second)\n",
      "llama_perf_context_print:       total time =     343.62 ms /     5 tokens\n",
      "Llama.generate: 559 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   21725.08 ms /   256 runs   (   84.86 ms per token,    11.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   22133.36 ms /   257 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy:  0.971\n",
      "Response likely grounded\n"
     ]
    }
   ],
   "source": [
    "prompt=final_prompt\n",
    "semantic_response=[model.create_completion(prompt=prompt, temperature=temp, max_tokens=256)['choices'][0]['text'] for _ in range(5)]\n",
    "\n",
    "entropy, labels=compute_semantic_entropy(semantic_response)\n",
    "print(f\"Entropy: {entropy: .3f}\")\n",
    "\n",
    "if entropy>1.5:\n",
    "    print(\"Possible hallucination.\")\n",
    "\n",
    "else:\n",
    "    print(\"Response likely grounded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_query():\n",
    "    user_query=input(\"What is your question: \")\n",
    "    response=model.create_completion(prompt=user_query, temperature=temp, max_tokens=256)['choices'][0]['text']\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95fee14",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mask_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m, in \u001b[0;36mask_query\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mask_query\u001b[39m():\n\u001b[0;32m      2\u001b[0m     user_query\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is your question: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     response\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcreate_completion(prompt\u001b[38;5;241m=\u001b[39muser_query, temperature\u001b[38;5;241m=\u001b[39mtemp, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "ask_query()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
