{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab09bb3",
   "metadata": {},
   "source": [
    "All necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c5c9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnab\\miniconda3\\envs\\ragEnv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "import llama_cpp\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a79533",
   "metadata": {},
   "source": [
    "Path of pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2010b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path=\"D:\\\\personalCode\\\\ragAgentFitness\\\\sample.pdf\"\n",
    "path=\"D:\\\\college books\\\\sem4\\\\CSD204 - OS\\\\textbook.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36529c",
   "metadata": {},
   "source": [
    "PDF text reading function, text splitting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1753748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_text(path):\n",
    "    doc=pymupdf.open(path)\n",
    "    full_text=\"\"\n",
    "    for page in doc:\n",
    "        full_text+=page.get_text()\n",
    "    return full_text\n",
    "\n",
    "def split_into_chunks(text, chunk_size=500, overlap=50):\n",
    "    chunks=[]\n",
    "    for i in range(0, len(text), chunk_size-overlap):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96185681",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=read_pdf_text(path)\n",
    "chunks=split_into_chunks(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478da314",
   "metadata": {},
   "source": [
    "Embedding and storage of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf9803b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnab\\miniconda3\\envs\\ragEnv\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedder=SentenceTransformer('all-MiniLM-L6-v2')\n",
    "vectors=embedder.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f05b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension=vectors[0].shape[0]\n",
    "index=faiss.IndexFlatL2(dimension)\n",
    "index.reset()\n",
    "index.add(np.array(vectors))\n",
    "\n",
    "id_to_text={i: chunk for i, chunk in enumerate(chunks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11de15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chunks(query, top_k=3):\n",
    "    query_vec=embedder.encode([query])\n",
    "    D, I=index.search(np.array(query_vec), top_k)\n",
    "    return [id_to_text[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d05f5f",
   "metadata": {},
   "source": [
    "Model being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e509a7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4060 Laptop GPU) - 7099 MiB free\n",
      "llama_model_loader: loaded meta data with 81 key-value pairs and 255 tensors from D:\\personalCode\\RAG-Toolkit\\models\\Dolphin3.0-Llama3.2-3B-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Dolphin 3.0 Llama 3.2 3B\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Cognitivecomputations\n",
      "llama_model_loader: - kv   4:                           general.basename str              = dolphin-3.0-Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Llama 3.2 3B\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Meta Llama\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...\n",
      "llama_model_loader: - kv  11:                      general.dataset.count u32              = 13\n",
      "llama_model_loader: - kv  12:                     general.dataset.0.name str              = Opc Sft Stage1\n",
      "llama_model_loader: - kv  13:             general.dataset.0.organization str              = OpenCoder LLM\n",
      "llama_model_loader: - kv  14:                 general.dataset.0.repo_url str              = https://huggingface.co/OpenCoder-LLM/...\n",
      "llama_model_loader: - kv  15:                     general.dataset.1.name str              = Opc Sft Stage2\n",
      "llama_model_loader: - kv  16:             general.dataset.1.organization str              = OpenCoder LLM\n",
      "llama_model_loader: - kv  17:                 general.dataset.1.repo_url str              = https://huggingface.co/OpenCoder-LLM/...\n",
      "llama_model_loader: - kv  18:                     general.dataset.2.name str              = Orca Agentinstruct 1M v1\n",
      "llama_model_loader: - kv  19:                  general.dataset.2.version str              = v1\n",
      "llama_model_loader: - kv  20:             general.dataset.2.organization str              = Microsoft\n",
      "llama_model_loader: - kv  21:                 general.dataset.2.repo_url str              = https://huggingface.co/microsoft/orca...\n",
      "llama_model_loader: - kv  22:                     general.dataset.3.name str              = Orca Math Word Problems 200k\n",
      "llama_model_loader: - kv  23:             general.dataset.3.organization str              = Microsoft\n",
      "llama_model_loader: - kv  24:                 general.dataset.3.repo_url str              = https://huggingface.co/microsoft/orca...\n",
      "llama_model_loader: - kv  25:                     general.dataset.4.name str              = Hermes Function Calling v1\n",
      "llama_model_loader: - kv  26:                  general.dataset.4.version str              = v1\n",
      "llama_model_loader: - kv  27:             general.dataset.4.organization str              = NousResearch\n",
      "llama_model_loader: - kv  28:                 general.dataset.4.repo_url str              = https://huggingface.co/NousResearch/h...\n",
      "llama_model_loader: - kv  29:                     general.dataset.5.name str              = NuminaMath CoT\n",
      "llama_model_loader: - kv  30:             general.dataset.5.organization str              = AI MO\n",
      "llama_model_loader: - kv  31:                 general.dataset.5.repo_url str              = https://huggingface.co/AI-MO/NuminaMa...\n",
      "llama_model_loader: - kv  32:                     general.dataset.6.name str              = NuminaMath TIR\n",
      "llama_model_loader: - kv  33:             general.dataset.6.organization str              = AI MO\n",
      "llama_model_loader: - kv  34:                 general.dataset.6.repo_url str              = https://huggingface.co/AI-MO/NuminaMa...\n",
      "llama_model_loader: - kv  35:                     general.dataset.7.name str              = Tulu 3 Sft Mixture\n",
      "llama_model_loader: - kv  36:             general.dataset.7.organization str              = Allenai\n",
      "llama_model_loader: - kv  37:                 general.dataset.7.repo_url str              = https://huggingface.co/allenai/tulu-3...\n",
      "llama_model_loader: - kv  38:                     general.dataset.8.name str              = Dolphin Coder\n",
      "llama_model_loader: - kv  39:             general.dataset.8.organization str              = Cognitivecomputations\n",
      "llama_model_loader: - kv  40:                 general.dataset.8.repo_url str              = https://huggingface.co/cognitivecompu...\n",
      "llama_model_loader: - kv  41:                     general.dataset.9.name str              = Smoltalk\n",
      "llama_model_loader: - kv  42:             general.dataset.9.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv  43:                 general.dataset.9.repo_url str              = https://huggingface.co/HuggingFaceTB/...\n",
      "llama_model_loader: - kv  44:                    general.dataset.10.name str              = Samantha Data\n",
      "llama_model_loader: - kv  45:            general.dataset.10.organization str              = Cognitivecomputations\n",
      "llama_model_loader: - kv  46:                general.dataset.10.repo_url str              = https://huggingface.co/cognitivecompu...\n",
      "llama_model_loader: - kv  47:                    general.dataset.11.name str              = CodeFeedback Filtered Instruction\n",
      "llama_model_loader: - kv  48:            general.dataset.11.organization str              = M A P\n",
      "llama_model_loader: - kv  49:                general.dataset.11.repo_url str              = https://huggingface.co/m-a-p/CodeFeed...\n",
      "llama_model_loader: - kv  50:                    general.dataset.12.name str              = Code Feedback\n",
      "llama_model_loader: - kv  51:            general.dataset.12.organization str              = M A P\n",
      "llama_model_loader: - kv  52:                general.dataset.12.repo_url str              = https://huggingface.co/m-a-p/Code-Fee...\n",
      "llama_model_loader: - kv  53:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  54:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv  55:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  56:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  57:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  58:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  59:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  60:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  61:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  62:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  63:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  64:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  65:                           llama.vocab_size u32              = 128258\n",
      "llama_model_loader: - kv  66:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  67:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  68:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  69:                      tokenizer.ggml.tokens arr[str,128258]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  70:                  tokenizer.ggml.token_type arr[i32,128258]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  71:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  72:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  73:                tokenizer.ggml.eos_token_id u32              = 128256\n",
      "llama_model_loader: - kv  74:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  75:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  76:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  77:                      quantize.imatrix.file str              = /models_out/Dolphin3.0-Llama3.2-3B-GG...\n",
      "llama_model_loader: - kv  78:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  79:             quantize.imatrix.entries_count i32              = 196\n",
      "llama_model_loader: - kv  80:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q5_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q5_K - Medium\n",
      "print_info: file size   = 2.16 GiB (5.76 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128257 '<|im_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 258\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 24\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 3\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.21 B\n",
      "print_info: general.name     = Dolphin 3.0 Llama 3.2 3B\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128258\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128256 '<|im_end|>'\n",
      "print_info: EOT token        = 128256 '<|im_end|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: PAD token        = 128001 '<|end_of_text|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: EOG token        = 128256 '<|im_end|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
      "load_tensors: offloading 28 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 29/29 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =  2207.11 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   308.24 MiB\n",
      ".............................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 8192\n",
      "llama_context: n_ctx_per_seq = 8192\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 8192 (padded)\n",
      "llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
      "llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 2\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:      CUDA0 compute buffer size =   424.00 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =    22.01 MiB\n",
      "llama_context: graph nodes  = 958\n",
      "llama_context: graph splits = 2\n",
      "CUDA : ARCHS = 890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.dataset.10.repo_url': 'https://huggingface.co/cognitivecomputations/samantha-data', 'general.name': 'Dolphin 3.0 Llama 3.2 3B', 'general.dataset.9.repo_url': 'https://huggingface.co/HuggingFaceTB/smoltalk', 'general.dataset.8.name': 'Dolphin Coder', 'general.architecture': 'llama', 'general.type': 'model', 'llama.context_length': '131072', 'general.organization': 'Cognitivecomputations', 'general.basename': 'dolphin-3.0-Llama-3.2', 'general.dataset.0.repo_url': 'https://huggingface.co/OpenCoder-LLM/opc-sft-stage1', 'general.dataset.12.name': 'Code Feedback', 'general.size_label': '3B', 'general.license': 'llama3.2', 'general.dataset.9.organization': 'HuggingFaceTB', 'general.base_model.count': '1', 'general.base_model.0.name': 'Llama 3.2 3B', 'general.base_model.0.organization': 'Meta Llama', 'llama.attention.value_length': '128', 'general.base_model.0.repo_url': 'https://huggingface.co/meta-llama/Llama-3.2-3B', 'general.dataset.2.name': 'Orca Agentinstruct 1M v1', 'general.dataset.11.organization': 'M A P', 'general.dataset.count': '13', 'llama.attention.key_length': '128', 'general.dataset.0.name': 'Opc Sft Stage1', 'llama.rope.freq_base': '500000.000000', 'general.dataset.5.repo_url': 'https://huggingface.co/AI-MO/NuminaMath-CoT', 'general.dataset.4.name': 'Hermes Function Calling v1', 'general.dataset.0.organization': 'OpenCoder LLM', 'general.dataset.1.name': 'Opc Sft Stage2', 'general.dataset.1.organization': 'OpenCoder LLM', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.dataset.1.repo_url': 'https://huggingface.co/OpenCoder-LLM/opc-sft-stage2', 'general.dataset.2.version': 'v1', 'general.dataset.2.organization': 'Microsoft', 'general.dataset.12.organization': 'M A P', 'general.dataset.2.repo_url': 'https://huggingface.co/microsoft/orca-agentinstruct-1M-v1', 'general.dataset.3.name': 'Orca Math Word Problems 200k', 'general.dataset.3.organization': 'Microsoft', 'general.dataset.3.repo_url': 'https://huggingface.co/microsoft/orca-math-word-problems-200k', 'general.dataset.10.organization': 'Cognitivecomputations', 'general.dataset.4.version': 'v1', 'general.dataset.11.name': 'CodeFeedback Filtered Instruction', 'general.dataset.4.organization': 'NousResearch', 'general.dataset.5.name': 'NuminaMath CoT', 'general.dataset.4.repo_url': 'https://huggingface.co/NousResearch/hermes-function-calling-v1', 'general.dataset.5.organization': 'AI MO', 'general.dataset.6.name': 'NuminaMath TIR', 'general.dataset.6.organization': 'AI MO', 'general.dataset.6.repo_url': 'https://huggingface.co/AI-MO/NuminaMath-TIR', 'general.dataset.7.name': 'Tulu 3 Sft Mixture', 'llama.feed_forward_length': '8192', 'general.dataset.7.organization': 'Allenai', 'general.dataset.7.repo_url': 'https://huggingface.co/allenai/tulu-3-sft-mixture', 'general.dataset.8.organization': 'Cognitivecomputations', 'general.dataset.8.repo_url': 'https://huggingface.co/cognitivecomputations/dolphin-coder', 'general.dataset.9.name': 'Smoltalk', 'general.dataset.10.name': 'Samantha Data', 'llama.block_count': '28', 'general.dataset.11.repo_url': 'https://huggingface.co/m-a-p/CodeFeedback-Filtered-Instruction', 'general.file_type': '17', 'llama.attention.head_count_kv': '8', 'tokenizer.ggml.eos_token_id': '128256', 'general.dataset.12.repo_url': 'https://huggingface.co/m-a-p/Code-Feedback', 'llama.embedding_length': '3072', 'llama.attention.head_count': '24', 'llama.vocab_size': '128258', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'general.quantization_version': '2', 'quantize.imatrix.file': '/models_out/Dolphin3.0-Llama3.2-3B-GGUF/Dolphin3.0-Llama3.2-3B.imatrix', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.entries_count': '196', 'quantize.imatrix.chunks_count': '125'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    }
   ],
   "source": [
    "model_path_gguf=\"D:\\\\personalCode\\\\RAG-Toolkit\\models\\\\Dolphin3.0-Llama3.2-3B-Q5_K_M.gguf\"\n",
    "model=llama_cpp.Llama(model_path=model_path_gguf, chat_format=\"llama-2\", n_ctx=8192, n_gpu_layers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c891863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n"
     ]
    }
   ],
   "source": [
    "print(model.context_params.n_ctx) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de4bad",
   "metadata": {},
   "source": [
    "User input and search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9565e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens used by the context: 321\n"
     ]
    }
   ],
   "source": [
    "user_query=input(\"Please enter query: \")\n",
    "context=\"\\n\\n\".join(search_chunks(user_query))\n",
    "\n",
    "context_tokens = model.tokenize(context.encode(\"utf-8\"))\n",
    "context_token_count = len(context_tokens)\n",
    "print(f\"Total tokens used by the context: {context_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d69f389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 409 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     778.50 ms\n",
      "llama_perf_context_print: prompt eval time =     321.40 ms /   409 tokens (    0.79 ms per token,  1272.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4968.82 ms /   284 runs   (   17.50 ms per token,    57.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    5578.42 ms /   693 tokens\n"
     ]
    }
   ],
   "source": [
    "final_prompt=f\"\"\"<|im_start|>system\n",
    "You are a helpful assistant. If the answer is not present in the context, print \"Insufficient context\" and nothing else. Structure your response in markdown, using bullet points or headings if appropriate. Ensure that if there is no relevant information, you provide \"Insufficient context\" and nothing else at all. <|im_end|>\n",
    "<|im_start|>user\n",
    "Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{user_query}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "temp=0.7\n",
    "max_tokens=2048\n",
    "\n",
    "response=model.create_completion(\n",
    "    prompt=final_prompt,\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb3713d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Deﬁning Operating Systems\n",
      "\n",
      "**Operating Systems:** An operating system is a type of software that manages the computer hardware and provides essential services for computer programs. It acts as an intermediary between the computer's hardware and the software applications running on it.\n",
      "\n",
      "**Key Functions:**\n",
      "1. **Resource Management:** Operating systems manage the allocation of resources like memory, storage, and processing power.\n",
      "2. **Input/Output (I/O) Control:** They manage the communication between the computer's hardware and the software applications, ensuring that the hardware operates correctly and that programs do not interfere with system operations.\n",
      "3. **Process Execution:** Operating systems control the execution of user programs, ensuring that they run efficiently and without causing errors or improper system use.\n",
      "4. **User Interface:** They provide the user interface through which users can interact with the computer system.\n",
      "5. **Error Handling:** Operating systems are designed to handle and prevent errors, ensuring that the computer system operates reliably.\n",
      "6. **Security:** They provide security mechanisms to protect the computer system from unauthorized access and misuse.\n",
      "\n",
      "**Design Variations:**\n",
      "Operating systems vary significantly in their design, structure, and capabilities. This diversity is due to the wide range of computing tasks, user requirements, and hardware characteristics. For example, some operating systems are designed for high performance in real-time applications, while others are optimized for power efficiency and resource usage in resource-constrained environments.\n"
     ]
    }
   ],
   "source": [
    "assistant_reply=response['choices'][0]['text']\n",
    "assistant_reply=assistant_reply.replace(\"[/INST]\", \"\")\n",
    "print(assistant_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1831027f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 605 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     778.50 ms\n",
      "llama_perf_context_print: prompt eval time =     336.30 ms /   605 tokens (    0.56 ms per token,  1799.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =     117.94 ms /     7 runs   (   16.85 ms per token,    59.35 tokens per second)\n",
      "llama_perf_context_print:       total time =     462.76 ms /   612 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant reply is not hallucinated.\n"
     ]
    }
   ],
   "source": [
    "hallucination_prompt = f\"\"\"<|im_start|>system\n",
    "You are an expert fact-checking assistant. Your job is to verify whether the assistant's answer is fully supported by the given context. If parts of the answer are not present in the context, clearly identify them. Be strict and objective in your judgment.<|im_end|>\n",
    "<|im_start|>user\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "{assistant_reply}\n",
    "\n",
    "Task:\n",
    "Determine whether the answer is hallucinated. List any parts of the answer that are not supported by the context. If answer is determined to not be hallucinated respond with \"Assistant reply is not hallucinated.\"<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "hal_response = model.create_completion(\n",
    "    prompt=hallucination_prompt,\n",
    "    temperature=temp,\n",
    "    max_tokens=256\n",
    ")\n",
    "assistant_hallucination = hal_response['choices'][0]['text']\n",
    "print(assistant_hallucination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76adb32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant reply is not hallucinated.\n"
     ]
    }
   ],
   "source": [
    "print(assistant_hallucination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd517867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1f47878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_semantic_entropy(responses, distance_threshold=1.0):\n",
    "    embeddings=embedder.encode(responses)\n",
    "\n",
    "    clustering=AgglomerativeClustering(n_clusters=None, distance_threshold=distance_threshold, linkage=\"average\")\n",
    "    labels=clustering.fit_predict(embeddings)\n",
    "\n",
    "    counts=Counter(labels)\n",
    "    total=sum(counts.values())\n",
    "    probabilities=[count/total for count in counts.values()]\n",
    "    entropy=-sum(p*math.log2(p) for p in probabilities)\n",
    "\n",
    "    return entropy, labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "003b2fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =     222.18 ms /     7 tokens (   31.74 ms per token,    31.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1908.22 ms /   138 runs   (   13.83 ms per token,    72.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    2259.40 ms /   145 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2588.46 ms /   187 runs   (   13.84 ms per token,    72.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    2765.17 ms /   188 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3565.64 ms /   256 runs   (   13.93 ms per token,    71.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    3819.42 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2641.84 ms /   192 runs   (   13.76 ms per token,    72.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    2818.49 ms /   193 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3562.41 ms /   256 runs   (   13.92 ms per token,    71.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    3820.55 ms /   257 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3236.62 ms /   223 runs   (   14.51 ms per token,    68.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    3451.05 ms /   224 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3557.23 ms /   256 runs   (   13.90 ms per token,    71.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    3799.40 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3130.27 ms /   226 runs   (   13.85 ms per token,    72.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    3343.91 ms /   227 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3581.14 ms /   256 runs   (   13.99 ms per token,    71.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    3795.97 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3554.61 ms /   254 runs   (   13.99 ms per token,    71.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    3757.32 ms /   255 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 1 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2982.39 ms /   214 runs   (   13.94 ms per token,    71.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    3152.50 ms /   215 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3584.33 ms /   256 runs   (   14.00 ms per token,    71.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    3784.38 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3585.11 ms /   256 runs   (   14.00 ms per token,    71.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    3784.94 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3578.12 ms /   256 runs   (   13.98 ms per token,    71.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    3782.99 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1223.05 ms /    89 runs   (   13.74 ms per token,    72.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1286.52 ms /    90 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 2 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2632.55 ms /   190 runs   (   13.86 ms per token,    72.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    2779.57 ms /   191 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2236.80 ms /   162 runs   (   13.81 ms per token,    72.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    2359.78 ms /   163 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3570.30 ms /   256 runs   (   13.95 ms per token,    71.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    3781.11 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3563.60 ms /   255 runs   (   13.97 ms per token,    71.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    3774.99 ms /   256 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1428.44 ms /   104 runs   (   13.74 ms per token,    72.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1503.04 ms /   105 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 3 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3253.38 ms /   234 runs   (   13.90 ms per token,    71.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    3436.15 ms /   235 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2197.26 ms /   159 runs   (   13.82 ms per token,    72.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    2314.72 ms /   160 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     176.75 ms /    13 runs   (   13.60 ms per token,    73.55 tokens per second)\n",
      "llama_perf_context_print:       total time =     185.56 ms /    14 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1317.13 ms /    96 runs   (   13.72 ms per token,    72.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1385.79 ms /    97 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1499.05 ms /   109 runs   (   13.75 ms per token,    72.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    1577.11 ms /   110 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 4 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1929.01 ms /   140 runs   (   13.78 ms per token,    72.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    2033.87 ms /   141 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2936.50 ms /   211 runs   (   13.92 ms per token,    71.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    3100.38 ms /   212 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1320.17 ms /    96 runs   (   13.75 ms per token,    72.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1388.05 ms /    97 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3578.36 ms /   256 runs   (   13.98 ms per token,    71.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    3788.03 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3572.97 ms /   256 runs   (   13.96 ms per token,    71.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    3777.72 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 5 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3230.95 ms /   232 runs   (   13.93 ms per token,    71.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    3414.69 ms /   233 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.82 ms /    84 runs   (   13.75 ms per token,    72.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1214.61 ms /    85 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3577.89 ms /   256 runs   (   13.98 ms per token,    71.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    3787.64 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3108.11 ms /   223 runs   (   13.94 ms per token,    71.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    3280.89 ms /   224 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3592.92 ms /   256 runs   (   14.03 ms per token,    71.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    3801.13 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 6 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2019.33 ms /   146 runs   (   13.83 ms per token,    72.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    2130.97 ms /   147 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1684.30 ms /   122 runs   (   13.81 ms per token,    72.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1775.65 ms /   123 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2079.33 ms /   150 runs   (   13.86 ms per token,    72.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    2192.38 ms /   151 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1984.70 ms /   143 runs   (   13.88 ms per token,    72.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    2088.58 ms /   144 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1401.14 ms /   101 runs   (   13.87 ms per token,    72.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1480.99 ms /   102 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 7 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2263.96 ms /   163 runs   (   13.89 ms per token,    72.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    2410.65 ms /   164 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3074.45 ms /   220 runs   (   13.97 ms per token,    71.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    3283.07 ms /   221 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3591.14 ms /   256 runs   (   14.03 ms per token,    71.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    3839.09 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3588.13 ms /   256 runs   (   14.02 ms per token,    71.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    3834.01 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3591.48 ms /   256 runs   (   14.03 ms per token,    71.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    3848.05 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 8 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3585.29 ms /   256 runs   (   14.01 ms per token,    71.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    3837.25 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3590.72 ms /   256 runs   (   14.03 ms per token,    71.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    3844.73 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3601.37 ms /   256 runs   (   14.07 ms per token,    71.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    3868.24 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3592.33 ms /   256 runs   (   14.03 ms per token,    71.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3854.36 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2516.77 ms /   180 runs   (   13.98 ms per token,    71.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    2682.76 ms /   181 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 9 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3607.52 ms /   256 runs   (   14.09 ms per token,    70.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    3875.55 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3598.81 ms /   256 runs   (   14.06 ms per token,    71.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    3855.79 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2300.62 ms /   165 runs   (   13.94 ms per token,    71.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    2456.06 ms /   166 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2984.05 ms /   213 runs   (   14.01 ms per token,    71.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    3190.29 ms /   214 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      14.86 ms /     1 runs   (   14.86 ms per token,    67.31 tokens per second)\n",
      "llama_perf_context_print:       total time =      16.48 ms /     2 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 10 Entropy:0.7219280948873623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     827.37 ms /    60 runs   (   13.79 ms per token,    72.52 tokens per second)\n",
      "llama_perf_context_print:       total time =     876.89 ms /    61 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      14.82 ms /     1 runs   (   14.82 ms per token,    67.46 tokens per second)\n",
      "llama_perf_context_print:       total time =      16.04 ms /     2 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3604.43 ms /   256 runs   (   14.08 ms per token,    71.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    3863.16 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3600.39 ms /   256 runs   (   14.06 ms per token,    71.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    3858.29 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1389.07 ms /   100 runs   (   13.89 ms per token,    71.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1477.25 ms /   101 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 11 Entropy:0.7219280948873623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3608.45 ms /   256 runs   (   14.10 ms per token,    70.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    3864.64 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1934.25 ms /   139 runs   (   13.92 ms per token,    71.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    2040.48 ms /   140 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3595.05 ms /   255 runs   (   14.10 ms per token,    70.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    3815.45 ms /   256 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1086.23 ms /    78 runs   (   13.93 ms per token,    71.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1145.08 ms /    79 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1939.02 ms /   139 runs   (   13.95 ms per token,    71.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    2046.50 ms /   140 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 12 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3605.01 ms /   256 runs   (   14.08 ms per token,    71.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    3849.74 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3605.75 ms /   256 runs   (   14.08 ms per token,    71.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    3855.38 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3606.30 ms /   256 runs   (   14.09 ms per token,    70.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    3854.07 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1469.25 ms /   106 runs   (   13.86 ms per token,    72.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1561.84 ms /   107 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3596.49 ms /   256 runs   (   14.05 ms per token,    71.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    3840.27 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 13 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3137.86 ms /   224 runs   (   14.01 ms per token,    71.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    3350.18 ms /   225 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3602.21 ms /   256 runs   (   14.07 ms per token,    71.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    3852.78 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2218.31 ms /   159 runs   (   13.95 ms per token,    71.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    2360.26 ms /   160 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2198.36 ms /   158 runs   (   13.91 ms per token,    71.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    2341.64 ms /   159 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3599.88 ms /   256 runs   (   14.06 ms per token,    71.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    3847.07 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 14 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3601.87 ms /   256 runs   (   14.07 ms per token,    71.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    3850.62 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2148.12 ms /   154 runs   (   13.95 ms per token,    71.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    2285.15 ms /   155 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3600.95 ms /   256 runs   (   14.07 ms per token,    71.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    3851.25 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3013.54 ms /   215 runs   (   14.02 ms per token,    71.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3212.89 ms /   216 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3598.86 ms /   256 runs   (   14.06 ms per token,    71.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    3854.68 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 15 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1439.23 ms /   104 runs   (   13.84 ms per token,    72.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1530.57 ms /   105 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3288.23 ms /   234 runs   (   14.05 ms per token,    71.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    3510.19 ms /   235 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3602.59 ms /   256 runs   (   14.07 ms per token,    71.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    3854.29 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2789.78 ms /   199 runs   (   14.02 ms per token,    71.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    2975.38 ms /   200 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3605.64 ms /   256 runs   (   14.08 ms per token,    71.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    3852.47 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 16 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3601.93 ms /   256 runs   (   14.07 ms per token,    71.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    3858.30 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2219.24 ms /   159 runs   (   13.96 ms per token,    71.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    2365.83 ms /   160 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1712.87 ms /   123 runs   (   13.93 ms per token,    71.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1821.86 ms /   124 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3604.09 ms /   256 runs   (   14.08 ms per token,    71.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    3844.66 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3275.12 ms /   233 runs   (   14.06 ms per token,    71.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    3497.38 ms /   234 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 17 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2354.40 ms /   169 runs   (   13.93 ms per token,    71.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    2501.80 ms /   170 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2819.26 ms /   201 runs   (   14.03 ms per token,    71.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    3006.78 ms /   202 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2800.15 ms /   200 runs   (   14.00 ms per token,    71.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    2983.04 ms /   201 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    1333.22 ms /    96 runs   (   13.89 ms per token,    72.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1414.91 ms /    97 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3596.49 ms /   256 runs   (   14.05 ms per token,    71.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    3836.26 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 18 Entropy:-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3366.06 ms /   240 runs   (   14.03 ms per token,    71.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    3557.72 ms /   241 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3598.86 ms /   256 runs   (   14.06 ms per token,    71.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    3803.10 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2656.86 ms /   190 runs   (   13.98 ms per token,    71.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    2802.41 ms /   191 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3604.93 ms /   256 runs   (   14.08 ms per token,    71.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    3828.84 ms /   257 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     737.43 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     907.49 ms /    66 runs   (   13.75 ms per token,    72.73 tokens per second)\n",
      "llama_perf_context_print:       total time =     953.45 ms /    67 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 19 Entropy:-0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    semantic_response=[model.create_completion(prompt=user_query, temperature=temp, max_tokens=256)['choices'][0]['text'] for _ in range(5)]\n",
    "    entropy, labels=compute_semantic_entropy(semantic_response, distance_threshold=1.0)\n",
    "    print(f\"At iteration {i} Entropy:{entropy}\")\n",
    "    if entropy>=1.5:\n",
    "        print(f\"iteration {i} detected possible hallucination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d0b2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 554 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =   14753.43 ms /   554 tokens (   26.63 ms per token,    37.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21498.60 ms /   255 runs   (   84.31 ms per token,    11.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   36652.24 ms /   809 tokens\n",
      "Llama.generate: 559 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     379.05 ms /     4 runs   (   94.76 ms per token,    10.55 tokens per second)\n",
      "llama_perf_context_print:       total time =     385.26 ms /     5 tokens\n",
      "Llama.generate: 559 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   21344.26 ms /   256 runs   (   83.38 ms per token,    11.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   21743.95 ms /   257 tokens\n",
      "Llama.generate: 559 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     338.12 ms /     4 runs   (   84.53 ms per token,    11.83 tokens per second)\n",
      "llama_perf_context_print:       total time =     343.62 ms /     5 tokens\n",
      "Llama.generate: 559 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   14894.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   21725.08 ms /   256 runs   (   84.86 ms per token,    11.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   22133.36 ms /   257 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy:  0.971\n",
      "Response likely grounded\n"
     ]
    }
   ],
   "source": [
    "prompt=final_prompt\n",
    "semantic_response=[model.create_completion(prompt=prompt, temperature=temp, max_tokens=256)['choices'][0]['text'] for _ in range(5)]\n",
    "\n",
    "entropy, labels=compute_semantic_entropy(semantic_response)\n",
    "print(f\"Entropy: {entropy: .3f}\")\n",
    "\n",
    "if entropy>1.5:\n",
    "    print(\"Possible hallucination.\")\n",
    "\n",
    "else:\n",
    "    print(\"Response likely grounded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_query():\n",
    "    user_query=input(\"What is your question: \")\n",
    "    response=model.create_completion(prompt=user_query, temperature=temp, max_tokens=256)['choices'][0]['text']\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95fee14",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mask_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m, in \u001b[0;36mask_query\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mask_query\u001b[39m():\n\u001b[0;32m      2\u001b[0m     user_query\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is your question: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     response\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcreate_completion(prompt\u001b[38;5;241m=\u001b[39muser_query, temperature\u001b[38;5;241m=\u001b[39mtemp, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "ask_query()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
